import os
import time
import requests
from tqdm import tqdm
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from seleniumwire import webdriver  # Using seleniumwire to capture network traffic
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

# -------------------------------
# SETUP & MANUAL LOGIN
# -------------------------------

# Selenium Wire options (empty in this example)
seleniumwire_options = {}

# Create Chrome options and set to detach the browser (so it remains open)
chrome_options = webdriver.ChromeOptions()
chrome_options.add_experimental_option("detach", True)

# Set up the Service using ChromeDriverManager
service = Service(ChromeDriverManager().install())

# Create the Selenium Wire webdriver
driver = webdriver.Chrome(
    service=service,
    seleniumwire_options=seleniumwire_options,
    options=chrome_options
)

# Open the Sin Chew Newsstand homepage
driver.get("") #INSERT TARGET WEBPAGE LINK HERE
time.sleep(5)  # Allow page to load

print("------------------------------------------------------")
print("Please log in manually in the opened browser window.")
print("After completing the login, come back here and type 'go' to continue.")
print("------------------------------------------------------")
input("Type 'go' then press Enter: ")

# -------------------------------
# NAVIGATE TO THE EDITION PAGE
# -------------------------------
try:
    newspaper_link = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, ".vc_link.vc_thumb_title_link"))
    )
    edition_url = newspaper_link.get_attribute("href")
    print("✅ Found newspaper edition:", edition_url)
    driver.get(edition_url)
    time.sleep(5)  # Allow the edition page to load
except Exception as e:
    print("⚠️ Failed to find newspaper edition:", e)
    driver.quit()
    exit()

# -------------------------------
# GET TOTAL NUMBER OF PAGES
# -------------------------------
try:
    # Extract the total page count from the element with class "vc_icon fa totalPages"
    total_pages_text = driver.find_element(By.CSS_SELECTOR, ".vc_icon.fa.totalPages").text
    total_pages = int(total_pages_text.strip())
    print("Total pages detected:", total_pages)
except Exception as e:
    print("⚠️ Unable to determine total number of pages. Using default value of 20.")
    total_pages = 20

# -------------------------------
# SETUP FOR IMAGE DOWNLOADS
# -------------------------------
# Extract edition name from the URL go
edition_name = edition_url.strip("/").split("/")[-1]

# Ensure folder name is valid
edition_name = edition_name.replace("?", "_").replace("&", "_").replace("=", "_")

# Create a dynamic folder based on edition name
folder_name = f"downloaded_images_{edition_name}"
os.makedirs(folder_name, exist_ok=True)

print(f"📁 Images will be saved in folder: {folder_name}")


# Create a persistent requests session with proper headers
session = requests.Session()
session.headers.update({
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/100.0.4896.127 Safari/537.36"),
    "Referer": " " #INSERT REFERER LINK HERE
})

def download_image(url, path, retries=3, delay=5):
    for attempt in range(retries):
        response = session.get(url, stream=True)
        # Debug: Uncomment the next line to see content length
        # print(f"Attempt {attempt+1}: Received {len(response.content)} bytes from {url}")
        if response.status_code == 200 and len(response.content) > 1000:
            with open(path, "wb") as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            return True
        else:
            print(f"⚠️ Attempt {attempt + 1} failed for {url} (Status: {response.status_code}, Bytes: {len(response.content)})")
            time.sleep(delay)
    return False

# -------------------------------
# SCROLL, CAPTURE, & DOWNLOAD IMAGES
# -------------------------------
processed_urls = set()
consecutive_no_new = 0
wait_time = 10  # seconds to wait after each scroll

print("Starting scroll-and-download loop...")

# Copy cookies from Selenium to requests session so the downloads are authenticated.
for cookie in driver.get_cookies():
    session.cookies.set(cookie['name'], cookie['value'])

# Use total_pages to limit iterations (or adjust as needed)
for i in range(total_pages):
    try:
        body = driver.find_element(By.TAG_NAME, "body")
        body.send_keys(Keys.RIGHT)
        print(f"Scrolled right (iteration {i+1}).")
    except Exception as e:
        print(f"Error sending RIGHT key on iteration {i+1}: {e}")
    
    time.sleep(wait_time)  # Allow images to load
    
    new_urls = []
    for request in driver.requests:
        if request.response and "zoompages" in request.url:
            if request.url not in processed_urls:
                new_urls.append(request.url)
                processed_urls.add(request.url)
    
    if new_urls:
        consecutive_no_new = 0
        print(f"Iteration {i+1}: Found {len(new_urls)} new image URLs.")
        for url in new_urls:
            filename = os.path.basename(url.split("?")[0])
            filepath = os.path.join(folder_name, filename)
            print("Downloading", url)
            if download_image(url, filepath):
                print("✅ Downloaded:", url)
            else:
                print("❌ Failed to download:", url)
    else:
        consecutive_no_new += 1
        print(f"Iteration {i+1}: No new image URLs found.")
    
    driver.requests.clear()
    
    if consecutive_no_new >= 3:
        print("No new images for 3 consecutive iterations. Stopping scroll.")
        break

driver.quit()
print("✅ All images downloaded!")
